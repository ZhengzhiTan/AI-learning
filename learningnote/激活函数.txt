激活函数
1. 激活函数所处的位置
在神经网络中，激活函数在每次下层函数上传到上层函数过程中出现，例如激活函数y = F(wX+b). 
2. 为什么需要激活函数：
个人理解是提高转递之中的复杂性，这里的复杂性主要是针对node与node之间的复杂性，如果没有有激活函数，nodes之间的传递的永远是线性关系。遇到更为复杂的数据模型，不会很理想。
3. 什么是激活函数：
大白话，就是一个方程一个函数，如sigmoid，tanh，Relu
4. 经典激活函数解析：
4.1 sigmod function
F(X) = 1/(1+e^(-x))
4.2Tanh/双曲正切激活函数
4.3 ReLU激活函数
4.4 Leaky ReLU
4.5Parametric ReLU激活函数、
4.6 ELU激活函数
4.7 SeLU激活函数
4.8 Softmax激活函数
4.9 Swish激活函数
4.10 Maxout激活函数
4.11 Softplus激活函数
5.如何选择激活函数
